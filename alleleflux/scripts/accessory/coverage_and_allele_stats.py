#!/usr/bin/env python3
"""
Compute mean per-position coverage across samples for MAGs.

This script processes all *_metadata.tsv files in a directory, where each metadata file
lists sample_id and file_path to each sample's profiled MAG file. It aggregates total_coverage
for each (contig, position) across all samples and reports mean coverage per position.

The script computes two sets of statistics for each position:
1. Present-only statistics: Averages computed only over samples where the position has coverage
2. Include-zeros statistics: Averages computed over all samples, treating absent positions as zero coverage

Optional QC filtering: Use --qc_dir to specify a directory containing QC files (*_QC.tsv)
generated by quality_control.py. Only samples that passed the breadth_threshold_passed
criterion will be included in the analysis.

**Memory-Efficient Operation:**
Two modes of operation are supported:
- Multi-MAG mode (default): Processes all MAGs using Python multiprocessing (higher memory usage)
- Single-MAG mode (--mag_id): Processes one MAG at a time for Snakemake parallelization (lower memory)

For large datasets (>50 MAGs), use single-MAG mode with Snakemake to significantly reduce
memory usage. See docs/source/usage/coverage_stats_memory.md for details.

Usage:
    # Multi-MAG mode (original behavior)
    alleleflux-mean-coverage --rootDir /path/to/metadata_files --output_dir /path/to/output --cpus 16

    # Single-MAG mode (memory-efficient, for Snakemake)
    alleleflux-mean-coverage --rootDir /path/to/metadata_files --output_dir /path/to/output --mag_id MAG001

    # With QC filtering
    alleleflux-mean-coverage --rootDir /path/to/metadata_files --output_dir /path/to/output --qc_dir /path/to/qc_files

Output:
A TSV per MAG with columns:
- contig, position, total_coverage, n_present, n_samples
- stats, std_coverage (present-only statistics)
- stats_with_zeros, std_coverage_with_zeros (include-zeros statistics)
- stats_group_<group>, std_coverage_group_<group> (present-only, if group column present)
- stats_with_zeros_group_<group>, std_coverage_with_zeros_group_<group> (include-zeros, if group column present)
- stats_group_<group>_time_<time>, std_coverage_group_<group>_time_<time> (present-only, if both group and time present)
- stats_with_zeros_group_<group>_time_<time>, std_coverage_with_zeros_group_<group>_time_<time> (include-zeros)
- n_present_group_<group> and n_present_group_<group>_time_<time> (number of samples with coverage)
- n_samples_group_<group> and n_samples_group_<group>_time_<time> (total number of samples in group)

Additionally, this script reports allele-frequency statistics (ATGC only) from the per-sample allele counts:
- mean_freq_<ALLELE> and std_freq_<ALLELE> (overall), where freq = <ALLELE> / total_coverage per sample and <ALLELE> in {A,C,G,T}
- Grouped counterparts for each present group/time combination, e.g., mean_freq_T_group_<g> and mean_freq_T_group_<g>_time_<t>

Important: Allele frequencies are always averaged only over samples with positive coverage at that position (present-only),
regardless of the coverage statistics mode, to avoid biasing frequencies toward zero.

Notes:
- Expects profile files to have columns: contig, position, total_coverage and allele columns A,C,G,T
- Duplicate (contig, position) within a file will raise an error
- Output sorted by contig, position
"""

import argparse
import functools
import logging
import multiprocessing
import os
import re
from glob import glob
from pathlib import Path
from typing import List, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

from alleleflux.scripts.utilities.logging_config import setup_logging

logger = logging.getLogger(__name__)

# Constants
ALLELES = ["A", "C", "G", "T"]


def read_mag_metadata(metadata_path: Path) -> pd.DataFrame:
    """
    Read and validate MAG metadata file containing sample information.

    The metadata file must be a TSV with at least 'sample_id' and 'file_path' columns.
    Optional columns include 'group' and 'time' for grouping samples.

    Args:
        metadata_path: Path to the TSV metadata file.

    Returns:
        A pandas DataFrame with normalized string columns.

    Raises:
        ValueError: If required columns are missing or the file is empty.
    """
    # Load the metadata from TSV
    df = pd.read_csv(metadata_path, sep="\t")
    # Check for required columns
    required = {"sample_id", "file_path"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(
            f"Missing required columns in MAG metadata '{metadata_path}': {sorted(missing)}"
        )
    # Normalize string types by stripping whitespace
    df["sample_id"] = df["sample_id"].astype(str).str.strip()
    df["file_path"] = df["file_path"].astype(str).str.strip()
    # Normalize optional grouping columns if present
    if "group" in df.columns:
        df["group"] = df["group"].astype(str).str.strip()
    if "time" in df.columns:
        df["time"] = df["time"].astype(str).str.strip()
    # Ensure not empty
    if df.empty:
        raise ValueError(f"No rows in MAG metadata file: {metadata_path}")
    return df


def read_profile(sample_file: str) -> pd.DataFrame:
    """
    Read one sample's coverage profile and return per-position coverage data.

    The profile file should be a TSV with columns: contig, position, total_coverage
    and allele count columns A,C,G,T for downstream allele-frequency statistics.

    Args:
        sample_file: Path to the sample's profile TSV file.

    Returns:
        A DataFrame with columns ['contig', 'position', 'total_coverage', 'A', 'C', 'G', 'T'].
        If the file has no data, returns an empty DataFrame with the expected columns.

    Raises:
        ValueError: If duplicate (contig, position) pairs exist in the file.
    """
    base_cols = ["contig", "position", "total_coverage"]
    usecols = base_cols + ALLELES

    logger.debug(f"Reading sample profile: {sample_file}")
    dtype = {"contig": str, "position": int, "total_coverage": float}
    dtype.update({allele: float for allele in ALLELES})

    df = pd.read_csv(sample_file, sep="\t", usecols=usecols, dtype=dtype)

    if df.empty:
        logger.warning(
            f"Profile file empty; sample excluded from present-only statistics: {sample_file}"
        )
        return pd.DataFrame(columns=usecols)

    dup = df.duplicated(subset=["contig", "position"], keep=False)
    if dup.any():
        raise ValueError(
            f"Duplicate (contig, position) pairs found in {sample_file}:\n"
            f"{df[dup].sort_values(['contig', 'position'])}"
        )
    return df


def read_qc_filtered_samples(qc_dir: str, mag_id: str):
    """
    Read QC file for a MAG and return set of sample_ids that passed breadth threshold.

    Args:
        qc_dir: Directory containing QC files from quality_control.py
        mag_id: MAG identifier to find the corresponding QC file

    Returns:
        Set of sample_ids that passed breadth_threshold_passed == True.
        Returns None if the QC file does not exist (indicating no filtering should be applied).
        Returns empty set if QC file exists but no samples passed (will result in no data after filtering).

    Raises:
        ValueError: If duplicate sample_ids are found in the QC file.
    """
    qc_file = Path(qc_dir) / f"{mag_id}_QC.tsv"

    if not qc_file.exists():
        logger.warning(f"QC file not found: {qc_file}. Will process all samples.")
        return None

    qc_df = pd.read_csv(
        qc_file, sep="\t", usecols=["sample_id", "breadth_threshold_passed"]
    )

    # Check for duplicated sample_ids and raise an error if found
    if qc_df["sample_id"].duplicated().any():
        raise ValueError(f"Duplicate sample_ids found in QC file {qc_file}")

    # Get samples that passed breadth threshold
    passed_samples = qc_df[qc_df["breadth_threshold_passed"] == True]["sample_id"]
    passed_set = set(passed_samples.astype(str).str.strip())

    # Warn if QC file exists but no samples passed
    if len(passed_set) == 0:
        logger.warning(
            f"QC file exists but contains no samples that passed breadth threshold for MAG {mag_id}. "
            f"Returning empty set - this MAG will have no data after filtering."
        )
        return set()

    logger.info(
        f"Found {len(passed_set)} samples that passed QC breadth threshold for MAG {mag_id}"
    )
    return passed_set


def load_and_combine_sample_data(meta: pd.DataFrame) -> pd.DataFrame:
    """
    Load and combine coverage data from all sample profile files listed in metadata.

    Args:
        meta: DataFrame with sample metadata including file_path column.
              Should be pre-filtered to only include desired samples (e.g., QC-passed).

    Returns:
        Combined DataFrame with all sample data tagged with sample_id, group, time
    """
    all_data = []

    # Iterate through each sample in the metadata
    for _, row in meta.iterrows():
        # Extract file path for this sample's profile
        fpath = Path(row["file_path"])
        # Skip and warn if the profile file does not exist (treat as all zeros)
        if not fpath.exists():
            logger.warning(f"Profile file not found; treated as all zeros: {fpath}")
            continue
        # Read per-position coverage data for this sample
        df = read_profile(fpath)
        # Skip if no coverage data in this sample
        if df.empty:
            continue
        # Tag the data with sample identifier
        df["sample_id"] = row["sample_id"]
        # Tag with group info if available in metadata
        if "group" in meta.columns:
            df["group"] = row["group"]
        # Tag with time info if available
        if "time" in meta.columns:
            df["time"] = row["time"]
        # Add this sample's data to the collection
        all_data.append(df)

    if not all_data:
        logger.warning("No coverage data found in any sample.")
        return pd.DataFrame()

    # Concatenate all sample DataFrames into a single comprehensive DataFrame
    return pd.concat(all_data, ignore_index=True)


def compute_allele_frequencies(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute allele frequencies and their squares for statistical calculations.

    Args:
        df: DataFrame with allele count columns A, C, G, T

    Returns:
        Modified DataFrame with frequency columns added
    """
    # Check for and warn about zero coverage positions
    zero_cov_count = (df["total_coverage"] == 0).sum()
    if zero_cov_count > 0:
        logger.warning(
            f"Found {zero_cov_count:,} positions with zero total_coverage - allele frequencies will be NaN"
        )

    # Flag rows with positive coverage; used to exclude zero-coverage rows
    # from present-only averages/stds while keeping include-zeros behavior explicit.
    df["has_cov"] = df["total_coverage"] > 0

    # Compute squared coverage for standard deviation calculations (using moment method)
    df["total_coverage_sq"] = df["total_coverage"] ** 2

    # Compute allele frequencies for ATGC
    for allele in ALLELES:
        df[f"{allele}_freq"] = df[allele].astype(float) / df["total_coverage"].where(
            df["total_coverage"] > 0, np.nan
        )
        df[f"{allele}_freq_sq"] = df[f"{allele}_freq"] ** 2

    return df


def compute_overall_statistics(df: pd.DataFrame, n_samples: int) -> pd.DataFrame:
    """
    Compute overall mean and standard deviation statistics across all samples.

    Args:
        df: Combined DataFrame with all sample data
        n_samples: Total number of samples (reflects only QC-passed samples if QC filtering enabled)

    Returns:
        DataFrame with overall statistics per position.

    Notes:
        - Computes two sets of coverage statistics:
            * stats/std_coverage: present-only (denominator = n_present)
            * stats_with_zeros/std_coverage_with_zeros: include-zeros (denominator = n_samples)
        - Allele frequencies (mean_freq_*, std_freq_*) are always computed using only the
          present samples as the denominator to avoid downward bias when many samples are absent.
    """

    logger.info("Calculating overall mean/std per position")
    # Aggregate coverage and allele frequency moments
    overall = _aggregate_with_freqs(
        df, ["contig", "position"], present_col_name="n_present", join_how="outer"
    )

    # Compute present-only statistics (original behavior)
    denom_present = overall["n_present"].astype(float)
    mean_present, std_present = _std_from_moments(
        overall["sum_cov"], overall["sumsq_cov"], denom_present
    )

    # Compute include-zeros statistics (new behavior)
    denom_with_zeros = float(n_samples)
    mean_with_zeros, std_with_zeros = _std_from_moments(
        overall["sum_cov"], overall["sumsq_cov"], denom_with_zeros
    )

    # Build result DataFrame with both sets of statistics
    tmp = overall.assign(
        total_coverage=overall["sum_cov"],  # Total coverage across all samples
        stats=mean_present,
        std_coverage=std_present,
        stats_with_zeros=mean_with_zeros,
        std_coverage_with_zeros=std_with_zeros,
        n_samples=n_samples,
        n_present=overall["n_present"].astype(int),
    )

    result = tmp.reset_index()[
        [
            "contig",
            "position",
            "total_coverage",
            "stats",
            "std_coverage",
            "stats_with_zeros",
            "std_coverage_with_zeros",
            "n_present",
            "n_samples",
        ]
    ]

    # Compute per-allele (ATGC) overall frequency stats
    # IMPORTANT: allele frequencies should be averaged over present samples only,
    # regardless of include_zeros mode to avoid biasing toward zero.
    denom_freq_overall = overall["n_present"].astype(float)
    allele_block_df = _compute_allele_means_stds(
        overall, denom_freq_overall
    ).reset_index()
    result = result.merge(allele_block_df, on=["contig", "position"], how="outer")

    return result


def compute_grouped_statistics(
    df: pd.DataFrame,
    meta: pd.DataFrame,
    result: pd.DataFrame,
    mag_id: str,
) -> pd.DataFrame:
    """
    Compute grouped statistics for samples with group and/or time information.

    Args:
        df: Combined DataFrame with all sample data
        meta: Metadata DataFrame (should be pre-filtered to only QC-passed samples if QC enabled)
        result: DataFrame with overall statistics to extend
        mag_id: MAG identifier for logging

    Returns:
        Extended DataFrame with grouped statistics.

    Notes:
        - Computes two sets of coverage statistics per group:
            * stats_group_X/std_coverage_group_X: present-only (denominator = n_present)
            * stats_with_zeros_group_X/std_coverage_with_zeros_group_X: include-zeros (denominator = group size)
        - Allele frequencies per group/time are always computed using only present samples in that
          group/time combination.
    """

    # Determine which grouping columns are present in metadata
    grouping_cols = [
        c for c in ("group", "time") if c in meta.columns and meta[c].nunique() > 1
    ]
    if not grouping_cols:
        logger.warning(
            "No grouping columns with multiple values found - skipping grouped statistics"
        )
        return result

    # Convert grouping columns to categorical for performance
    for c in grouping_cols:
        df[c] = df[c].astype("category")
        meta[c] = meta[c].astype("category")

    # Iterate over grouping levels: first 'group', then 'group'+'time' if both
    for i in range(len(grouping_cols)):
        current_groups = grouping_cols[: i + 1]
        logger.info(f"Grouped stats for: {current_groups} for MAG {mag_id}")

        # Group by contig, position, and group(s), aggregate sums and counts
        idx = ["contig", "position", *current_groups]
        grouped = _aggregate_with_freqs(
            df, idx, present_col_name="n_present_group", join_how="outer"
        )

        # Get total group sizes from metadata for denominators
        group_sizes = meta.groupby(current_groups, observed=True).size()

        # n_present_used always reflects actual samples with positive coverage
        n_present_used = grouped["n_present_group"]

        # Calculate present-only statistics
        denom_present = grouped["n_present_group"].astype(float)
        means_present, stds_present = _std_from_moments(
            grouped["sum_cov"], grouped["sumsq_cov"], denom_present
        )

        # Calculate include-zeros statistics
        # Map group sizes to each row in grouped using the group levels from the MultiIndex
        group_size_per_row = grouped.index.to_frame()[current_groups].apply(
            lambda row: (
                group_sizes[tuple(row)]
                if len(current_groups) > 1
                else group_sizes[row.iloc[0]]
            ),
            axis=1,
        )
        denom_with_zeros = pd.Series(
            group_size_per_row.values, index=grouped.index, dtype=float
        )
        means_with_zeros, stds_with_zeros = _std_from_moments(
            grouped["sum_cov"], grouped["sumsq_cov"], denom_with_zeros
        )

        # Pivot all statistics to wide format for each (contig, position)
        wide_mean = means_present.unstack(current_groups)
        wide_std = stds_present.unstack(current_groups)
        wide_mean_with_zeros = means_with_zeros.unstack(current_groups)
        wide_std_with_zeros = stds_with_zeros.unstack(current_groups)
        wide_npr = n_present_used.unstack(current_groups)

        ns_series = pd.Series(
            group_size_per_row.values,
            index=grouped.index,
            name="n_samples_group",
        )
        wide_ns = ns_series.unstack(current_groups)

        # Rename columns to descriptive names
        mean_cols = _flatten_cols("stats_", current_groups, wide_mean.columns)
        std_cols = _flatten_cols("std_coverage_", current_groups, wide_std.columns)
        mean_with_zeros_cols = _flatten_cols(
            "stats_with_zeros_", current_groups, wide_mean_with_zeros.columns
        )
        std_with_zeros_cols = _flatten_cols(
            "std_coverage_with_zeros_", current_groups, wide_std_with_zeros.columns
        )
        npr_cols = _flatten_cols("n_present_", current_groups, wide_npr.columns)
        ns_cols = _flatten_cols("n_samples_", current_groups, wide_ns.columns)

        # Apply the new column names
        wide_mean.columns = mean_cols
        wide_std.columns = std_cols
        wide_mean_with_zeros.columns = mean_with_zeros_cols
        wide_std_with_zeros.columns = std_with_zeros_cols
        wide_npr.columns = npr_cols
        wide_ns.columns = ns_cols

        # Collect all blocks for efficient concatenation
        blocks = [
            wide_mean,
            wide_std,
            wide_mean_with_zeros,
            wide_std_with_zeros,
            wide_npr,
            wide_ns,
        ]

        # Compute grouped per-allele frequency stats and add to blocks
        # IMPORTANT: allele frequencies should be averaged over present samples only
        denom_freq_grouped = grouped["n_present_group"].astype(float)
        allele_stats = _compute_allele_means_stds(grouped, denom_freq_grouped)
        for allele in ALLELES:
            mean_col = f"mean_freq_{allele}"
            std_col = f"std_freq_{allele}"
            wide_mean_freq_allele = allele_stats[mean_col].unstack(current_groups)
            wide_std_freq_allele = allele_stats[std_col].unstack(current_groups)
            wide_mean_freq_allele.columns = _flatten_cols(
                f"mean_freq_{allele}_", current_groups, wide_mean_freq_allele.columns
            )
            wide_std_freq_allele.columns = _flatten_cols(
                f"std_freq_{allele}_", current_groups, wide_std_freq_allele.columns
            )
            blocks.append(wide_mean_freq_allele)
            blocks.append(wide_std_freq_allele)

        # Combine all blocks efficiently with concat and merge back
        block = pd.concat(blocks, axis=1).reset_index()
        result = result.merge(block, on=["contig", "position"], how="left")

    return result


# --- helpers: small, focused utilities ---------------------------------------
def _std_from_moments(sum_, sumsq, denom) -> Tuple[pd.Series, pd.Series]:
    """Compute mean and standard deviation from first and second moments.

    This uses the population variance identity Var(X) = E[X^2] - (E[X])^2.
    Inputs can be pandas Series/Index-aligned arrays or numpy arrays; ``denom``
    may be a scalar or a vector aligned to ``sum_``/``sumsq``.

    To avoid tiny negative values due to floating-point error, the variance is
    clamped at zero via ``np.maximum(var, 0.0)`` before taking the square root.

    Parameters:
        sum_ (pd.Series | np.ndarray | float): Sum of values per key.
        sumsq (pd.Series | np.ndarray | float): Sum of squared values per key.
        denom (pd.Series | np.ndarray | float): Denominator (count) used for the
            mean/std calculation. Can be a scalar (e.g., total sample size) or a
            vector per key (e.g., n_present per position/group).

    Returns:
        Tuple[pd.Series, pd.Series]: A pair (mean, std) computed elementwise.

    Examples:
        >>> import pandas as pd
        >>> sum_ = pd.Series([10.0, 6.0])
        >>> sumsq = pd.Series([110.0, 20.0])
        >>> denom = pd.Series([2.0, 2.0])
        >>> mean, std = _std_from_moments(sum_, sumsq, denom)
        >>> round(mean.iloc[0], 3), round(std.iloc[0], 3)
        (5.0, 5.477)
        >>> round(mean.iloc[1], 3), round(std.iloc[1], 3)
        (3.0, 1.0)

        Edge case with zero denominator yields NaN:
        >>> mean2, std2 = _std_from_moments(pd.Series([0.0]), pd.Series([0.0]), 0.0)
        >>> float(mean2.iloc[0]), float(std2.iloc[0])
        (nan, nan)
    """
    mean = sum_ / denom
    var = (sumsq / denom) - mean**2
    return mean, np.sqrt(np.maximum(var, 0.0))


def _sanitize(x: object) -> str:
    """Sanitize labels for safe column naming.

    Replaces any character not in [A-Za-z0-9_.-] with an underscore. Input is
    first converted to ``str``. Useful for turning group/time labels into clean
    column suffixes.

    Parameters:
        x (object): Any value representing a label.

    Returns:
        str: A sanitized string with only [A-Za-z0-9_.-] and underscores.

    Examples:
        >>> _sanitize("Group A / time-1")
        'Group_A_time-1'
        >>> _sanitize(42)
        '42'
        >>> _sanitize("a:b(c)d")
        'a_b_c_d'
    """
    return re.sub(r"[^A-Za-z0-9_.-]+", "_", str(x))


def _flatten_cols(prefix: str, current_groups: List[str], cols) -> List[str]:
    """Convert a MultiIndex-like list of group labels into flat column names.

    If an element of ``cols`` is a tuple (e.g., from unstacked MultiIndex), it is
    paired with the corresponding group names from ``current_groups`` to form
    parts like ``group_A`` and ``time_1``. If it is a scalar label, only the first
    entry in ``current_groups`` is used. All labels are sanitized by ``_sanitize``.

    Parameters:
        prefix (str): Column name prefix to start with, e.g., 'stats_'.
        current_groups (List[str]): Ordered list of group keys, e.g., ['group', 'time'].
        cols (Iterable[Any]): Labels from an unstacked index, either tuples or scalars.

    Returns:
        List[str]: Flattened column names of the form
            '<prefix>group_<g>_time_<t>' or '<prefix>group_<g>' when a single
            grouping dimension is present.

    Examples:
        Single grouping level:
        >>> _flatten_cols('stats_', ['group'], ['A', 'B'])
        ['stats_group_A', 'stats_group_B']

        Two grouping levels (tuples expected in cols):
        >>> _flatten_cols('std_coverage_', ['group', 'time'], [('A', '1'), ('B', '2')])
        ['std_coverage_group_A_time_1', 'std_coverage_group_B_time_2']
    """
    out = []
    for lab in cols:
        if isinstance(lab, tuple):
            parts = [f"{c}_{_sanitize(v)}" for c, v in zip(current_groups, lab)]
        else:
            parts = [f"{current_groups[0]}_{_sanitize(lab)}"]
        out.append(prefix + "_".join(parts))
    return out


def _aggregate_with_freqs(
    df: pd.DataFrame,
    idx: List[str],
    present_col_name: str,
    join_how: str = "outer",
) -> pd.DataFrame:
    """Aggregate coverage moments and allele-frequency moments on an index.

    Produces sum of coverage, sum of squared coverage, and count of present samples
    (based on ``has_cov``). Additionally joins sums of allele frequencies and their
    squares for each allele in ALLELES.

    Parameters:
        df: Long-form DataFrame containing 'total_coverage', 'total_coverage_sq',
            'has_cov', and per-allele frequency columns '<A>_freq' and
            '<A>_freq_sq' for A in ALLELES.
        idx: Grouping columns to aggregate by, e.g., ['contig', 'position'] or
            ['contig', 'position', 'group'].
        present_col_name: Name to use for the present-count column in the result
            (e.g., 'n_present' or 'n_present_group').
        join_how: How to join frequency aggregates onto coverage aggregates; kept
            as 'outer' by default to match existing behavior.

    Returns:
        Aggregated DataFrame indexed by ``idx`` with columns:
        ['sum_cov', 'sumsq_cov', present_col_name, '<A>_freq_sum', '<A>_freq_sumsq', ...]
    """
    cov_agg = (
        df.groupby(idx, observed=True)
        .agg(
            sum_cov=("total_coverage", "sum"),
            sumsq_cov=("total_coverage_sq", "sum"),
            present=("has_cov", "sum"),
        )
        .sort_index()
    )
    cov_agg = cov_agg.rename(columns={"present": present_col_name})

    # Build allele frequency aggregation dict
    agg_add = {}
    for allele in ALLELES:
        agg_add[f"{allele}_freq_sum"] = (f"{allele}_freq", "sum")
        agg_add[f"{allele}_freq_sumsq"] = (f"{allele}_freq_sq", "sum")

    freq_agg = df.groupby(idx, observed=True).agg(**agg_add).sort_index()
    return cov_agg.join(freq_agg, how=join_how)


def _compute_allele_means_stds(
    agg: pd.DataFrame, denom, alleles: List[str] = ALLELES
) -> pd.DataFrame:
    """Compute per-allele mean and std from aggregated frequency moments.

    Parameters:
        agg: Aggregated DataFrame containing '<A>_freq_sum' and '<A>_freq_sumsq'.
        denom: Denominator (scalar or aligned Series) used for moments.
        alleles: List of allele labels to compute stats for.

    Returns:
        DataFrame with columns 'mean_freq_<A>' and 'std_freq_<A>' for each allele,
        indexed like ``agg``.
    """
    out = pd.DataFrame(index=agg.index)
    for allele in alleles:
        mean_allele_freq, std_allele_freq = _std_from_moments(
            agg[f"{allele}_freq_sum"], agg[f"{allele}_freq_sumsq"], denom
        )
        out[f"mean_freq_{allele}"] = mean_allele_freq
        out[f"std_freq_{allele}"] = std_allele_freq
    return out


# --- main computation function ------------------------------------------------


def compute_stats(mag_metadata: Path, qc_dir: str = None) -> pd.DataFrame:
    """
    Computes mean and standard deviation coverage statistics for a single MAG.

    This function orchestrates the computation of coverage statistics by:
    1. Loading and combining sample data from multiple profile files
    2. Computing allele frequencies for statistical analysis
    3. Calculating overall statistics across all samples
    4. Computing grouped statistics if group/time metadata is available

    Args:
        mag_metadata: Path to the MAG's metadata TSV file.
        qc_dir: Optional directory containing QC files (*_QC.tsv) from quality_control.py.
            If provided, only samples that passed breadth_threshold_passed will be included.
            n_samples will reflect only QC-passed samples in all calculations.

    Returns:
        A pandas DataFrame containing the mean coverage and allele frequency statistics,
        with columns for contig, position, overall statistics (both present-only and include-zeros),
        and group-specific statistics.
    """
    # Read and validate the MAG metadata file
    meta = read_mag_metadata(mag_metadata)
    mag_id = os.path.basename(mag_metadata).split("_metadata")[0]

    # Apply QC filtering if requested
    qc_filtered_samples = None
    if qc_dir:
        qc_filtered_samples = read_qc_filtered_samples(qc_dir, mag_id)

    # Filter metadata to only include QC-passed samples
    # Only filter if qc_filtered_samples is not None (i.e., QC file exists and has passing samples)
    if qc_filtered_samples is not None:
        # Convert sample_id to string and strip for comparison
        meta_sample_ids = meta["sample_id"].astype(str).str.strip()
        meta = meta[meta_sample_ids.isin(qc_filtered_samples)].copy()
        logger.info(
            f"Metadata filtered to {len(meta)} samples that passed QC for MAG {mag_id}"
        )
        # Check if filtering resulted in no samples
        if len(meta) == 0:
            logger.warning(
                f"No samples remaining after QC filtering for MAG {mag_id}. "
                f"Returning empty result."
            )

    n_samples = len(meta)
    logger.info(f"Combining profiles for MAG: {mag_id}")
    # Load and combine all sample data (metadata already filtered to QC-passed samples)
    df = load_and_combine_sample_data(meta)
    logger.info(f"Combined data for MAG {mag_id} shape: {df.shape}")

    # Handle case where no data was found
    if df.empty:
        logger.warning("No coverage data found in any sample. Returning empty result.")
        return pd.DataFrame(
            columns=[
                "contig",
                "position",
                "total_coverage",
                "stats",
                "std_coverage",
                "n_present",
                "n_samples",
            ]
        )

    # Compute allele frequencies and prepare data for statistical calculations
    logger.info(f"Computing allele frequencies for MAG: {mag_id}")
    df = compute_allele_frequencies(df)
    logger.info("Computing both present-only and include-zeros statistics")

    # Compute overall statistics across all samples
    result = compute_overall_statistics(df, n_samples)

    # Compute grouped statistics if group/time information is available
    result = compute_grouped_statistics(df, meta, result, mag_id)

    # Finalize and return results
    base = [
        "contig",
        "position",
        "total_coverage",
        "stats",
        "std_coverage",
        "stats_with_zeros",
        "std_coverage_with_zeros",
        "n_present",
        "n_samples",
    ]

    # Collect all group-related columns dynamically by prefix
    groupish = [
        col
        for col in result.columns
        if col.startswith(
            (
                "stats_",  # total coverage grouped means
                "std_coverage_",  # total coverage grouped stds
                "stats_with_zeros_",  # with zeros grouped means
                "std_coverage_with_zeros_",  # with zeros grouped stds
                "n_present_",
                "n_samples_",
                "mean_freq_",
                "std_freq_",
            )
        )
        and col
        not in {
            "stats",
            "std_coverage",
            "stats_with_zeros",
            "std_coverage_with_zeros",
        }
    ]

    # Combine base and group columns, sort by contig and position, round to 6 decimals
    out = result[base + groupish].sort_values(["contig", "position"]).round(6)
    return out


def process_metadata_file_worker(
    metadata_file: str, output_dir: str, qc_dir: str = None
) -> str:
    """
    Worker function to process a single metadata file in parallel.

    This function is designed to be called by multiprocessing.Pool workers.
    It processes one MAG metadata file, computes coverage statistics, and saves the output.

    Args:
        metadata_file: Path to the metadata TSV file for a single MAG.
        output_dir: Directory where output files should be saved.
        qc_dir: Optional directory containing QC files for filtering samples.

    """
    mag_id = os.path.basename(metadata_file).split("_metadata")[0]

    logger.info(f"Processing MAG: {mag_id}")

    # Compute mean coverage for this MAG
    df = compute_stats(
        mag_metadata=Path(metadata_file),
        qc_dir=qc_dir,
    )

    # Save output file
    out_file = Path(output_dir) / f"{mag_id}_stats.tsv"
    df.to_csv(out_file, sep="\t", index=False)

    logger.info(
        f"Saved mean coverage ({df.shape[0]:,} rows) for {mag_id} to {out_file}"
    )

    return str(out_file)


def main():
    """
    Main entry point for the script.

    Parses command-line arguments, finds all *_metadata.tsv files in the input directory,
    and processes each MAG to compute mean coverage per position. Outputs are saved as
    TSV files in the specified output directory.

    This function uses multiprocessing to process multiple metadata files concurrently,
    utilizing all available CPU cores for improved performance when processing many files.
    """
    # Initialize logging for the module
    setup_logging()
    # Set up command-line argument parser
    args = argparse.ArgumentParser(
        description=(
            "Calculate mean per-position coverage across samples for MAGs "
            "using per-MAG metadata TSV files."
        ),
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    args.add_argument(
        "--rootDir",
        required=True,
        type=str,
        help="Directory containing per-MAG *_metadata.tsv files.",
    )
    args.add_argument(
        "--output_dir",
        required=True,
        type=str,
        help="Directory to write output TSV files.",
    )
    args.add_argument(
        "--qc_dir",
        type=Path,
        help=(
            "Directory containing QC files (*_QC.tsv) from quality_control.py. "
            "If provided, only samples that passed breadth_threshold_passed will be included in analysis."
        ),
    )
    args.add_argument(
        "--cpus",
        type=int,
        default=multiprocessing.cpu_count(),
        help=(
            f"Number of CPU cores to use for parallel processing, all available cores)."
        ),
    )
    args.add_argument(
        "--mag_id",
        type=str,
        help=(
            "Process only a single MAG by ID (e.g., 'MAG001'). "
            "If specified, only {mag_id}_metadata.tsv will be processed. "
            "Use this option for memory-efficient per-MAG processing via Snakemake parallelization."
        ),
    )
    args = args.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)

    # Single MAG mode (memory-efficient, for Snakemake parallelization)
    if args.mag_id:
        metadata_file = os.path.join(args.rootDir, f"{args.mag_id}_metadata.tsv")
        if not os.path.exists(metadata_file):
            logger.error(f"Metadata file not found: {metadata_file}")
            return

        logger.info(f"Processing single MAG: {args.mag_id}")
        process_metadata_file_worker(
            metadata_file=metadata_file,
            output_dir=args.output_dir,
            qc_dir=args.qc_dir,
        )
        logger.info(f"Completed processing MAG: {args.mag_id}")
        return

    # Multi-MAG mode (original behavior with multiprocessing)
    metadata_files = glob(os.path.join(args.rootDir, "*_metadata.tsv"))
    if not metadata_files:
        logger.error(
            f"No *_metadata.tsv files found in input directory: {args.rootDir}"
        )
        return

    num_files = len(metadata_files)
    # Ensure we don't use more CPUs than files to process
    num_cpus = min(args.cpus, num_files)

    logger.info(f"Found {num_files} metadata file(s) to process")
    logger.info(f"Using {num_cpus} CPU(s) for parallel processing")

    # Validate QC directory if provided
    if args.qc_dir and not os.path.isdir(args.qc_dir):
        logger.error(f"QC directory does not exist: {args.qc_dir}")
        return

    if args.qc_dir:
        logger.info(f"QC filtering enabled using directory: {args.qc_dir}")
    else:
        logger.info("QC filtering disabled - processing all samples")

    # Parallel path with unordered completion and progress bar
    worker = functools.partial(
        process_metadata_file_worker,
        output_dir=args.output_dir,
        qc_dir=args.qc_dir,
    )
    with multiprocessing.Pool(processes=num_cpus) as pool:
        for _ in tqdm(
            pool.imap_unordered(worker, metadata_files),
            total=num_files,
            desc="Processing MAGs",
            unit="file",
        ):
            pass


if __name__ == "__main__":
    main()
